{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\"\"\"Documentação da Análise de Classes Gramaticais no MacMorpho\"\"\"\n",
        "\n",
        "# Introdução\n",
        "Este notebook descreve a tarefa, metodologia e os resultados obtidos na análise de classes gramaticais utilizando um modelo de classificação baseado em Transformers. O objetivo principal é avaliar a precisão de diferentes classes gramaticais em uma tarefa de etiquetagem POS (Part-of-Speech).\n",
        "\n",
        "# Descrição da Tarefa\n",
        "A tarefa consiste em treinar e avaliar um modelo para etiquetar palavras em sentenças com suas respectivas classes gramaticais. O dataset utilizado é o MacMorpho, amplamente utilizado para o processamento de linguagem natural em português. A análise inclui:\n",
        "\n",
        "1. Treinamento de um modelo baseado no BERT pré-treinado (\"neuralmind/bert-base-portuguese-cased\").\n",
        "2. Avaliação do modelo no conjunto de teste.\n",
        "3. Geração de métricas por classe gramatical.\n",
        "\n",
        "# Metodologia\n",
        "\n",
        "## Dados\n",
        "\n",
        "Os dados foram obtidos do dataset MacMorpho, e foram divididos em:\n",
        "- Conjunto de treinamento.\n",
        "- Conjunto de teste.\n",
        "\n",
        "Cada palavra no dataset é associada a uma tag que representa sua classe gramatical. Essas classes incluem substantivos (N), verbos (V), adjetivos (ADJ), entre outros.\n",
        "\n",
        "### Pré-processamento\n",
        "Os dados foram pré-processados para:\n",
        "- Tokenizar as palavras utilizando o tokenizer do BERT.\n",
        "- Mapear tags para índices e vice-versa.\n",
        "- Gerar entradas no formato compatível com modelos da biblioteca Transformers.\n",
        "\n",
        "## Modelo\n",
        "\n",
        "Utilizou-se o modelo \"neuralmind/bert-base-portuguese-cased\", adaptado para classificação de tokens com o número de labels correspondente às classes gramaticais do MacMorpho. O modelo foi treinado com:\n",
        "- 3 épocas.\n",
        "- Batch size de 8.\n",
        "- Taxa de aprendizado inicial de 5e-5.\n",
        "\n",
        "### Métricas de Avaliação\n",
        "As métricas utilizadas para avaliar o modelo incluem:\n",
        "- Precisão (Precision)\n",
        "- Revocação (Recall)\n",
        "- F1-Score\n",
        "\n",
        "Além disso, foram calculadas métricas por classe gramatical.\n",
        "\n",
        "# Resultados\n",
        "\n",
        "## Métricas Gerais\n",
        "As métricas gerais obtidas foram:\n",
        "```python\n",
        "metrics = trainer.evaluate()\n",
        "print(\"Precisão geral:\", metrics[\"precision\"])\n",
        "print(\"Revocação geral:\", metrics[\"recall\"])\n",
        "print(\"F1 geral:\", metrics[\"f1\"])\n",
        "```\n",
        "\n",
        "## Análise por Classe Gramatical\n",
        "Para avaliar o desempenho por classe, foi gerado um relatório detalhado:\n",
        "\n",
        "```python\n",
        "def evaluate_model_per_class(model, tokenizer, X_test, y_test, label_classes):\n",
        "    # Implementação detalhada no código abaixo\n",
        "\n",
        "report = evaluate_model_per_class(model, tokenizer, X_test, y_test, test_data.unique_tags)\n",
        "print(report)\n",
        "```\n",
        "\n",
        "### Classes com Maior Precisão\n",
        "Com base na análise, as classes gramaticais com maior precisão foram:\n",
        "- Substantivos (N)\n",
        "- Verbos (V)\n",
        "\n",
        "### Classes com Menor Precisão\n",
        "As classes com menor precisão incluíram:\n",
        "- Advérbios (ADV)\n",
        "- Preposições (PRP)\n",
        "\n",
        "# Conclusão\n",
        "O modelo baseado em BERT demonstrou bom desempenho geral, mas apresentou variação significativa entre as classes gramaticais. Classes mais frequentes no dataset, como substantivos e verbos, tendem a obter maior precisão. Por outro lado, classes menos frequentes, como advérbios, apresentam menor precisão, indicando a necessidade de mais dados ou ajustes para essas classes específicas.\n",
        "\n",
        "# Próximos Passos\n",
        "- Experimentar diferentes arquiteturas de modelos.\n",
        "- Realizar fine-tuning com mais épocas ou técnicas de data augmentation.\n",
        "- Investigar as classes com baixa precisão para entender as causas (por exemplo, ambiguidade linguística ou escassez de exemplos no dataset)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "FmLDRyS6FYo9",
        "outputId": "13bf637b-e572-496a-eab1-10da52f97df1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2025-01-19 19:42:15--  http://nilc.icmc.usp.br/macmorpho/macmorpho-v3.tgz\n",
            "Resolving nilc.icmc.usp.br (nilc.icmc.usp.br)... 143.107.183.225\n",
            "Connecting to nilc.icmc.usp.br (nilc.icmc.usp.br)|143.107.183.225|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2463485 (2.3M) [application/x-gzip]\n",
            "Saving to: ‘macmorpho-v3.tgz’\n",
            "\n",
            "macmorpho-v3.tgz    100%[===================>]   2.35M  1.90MB/s    in 1.2s    \n",
            "\n",
            "2025-01-19 19:42:17 (1.90 MB/s) - ‘macmorpho-v3.tgz’ saved [2463485/2463485]\n",
            "\n",
            "macmorpho-dev.txt\n",
            "macmorpho-test.txt\n",
            "macmorpho-train.txt\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "main_file_path = \"data/macmorpho-v3.tgz\"\n",
        "test_filepath = \"data/macmorpho-test.txt\"\n",
        "train_filepath = \"data/macmorpho-train.txt\"\n",
        "\n",
        "if not os.path.exists(main_file_path):\n",
        "    os.makedirs(\"data\", exist_ok=True)\n",
        "    os.system(\"wget -P data/ http://nilc.icmc.usp.br/macmorpho/macmorpho-v3.tgz\")\n",
        "    os.system(\"tar -xvzf data/macmorpho-v3.tgz -C data/\")\n",
        "    print(\"Dataset imported\\n\\n\")\n",
        "else:\n",
        "    print(\"Dataset was already imported\\n\\n\")\n",
        "\n",
        "os.system(\"pip install 'accelerate>=0.26.0'\")\n",
        "os.system(\"pip install transformers seqeval\")\n",
        "\n",
        "print(\"Pip done\\n\\n\")\n",
        "\n",
        "import os\n",
        "from tqdm import tqdm_notebook as tqdm\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForTokenClassification,\n",
        "    DataCollatorForTokenClassification,\n",
        "    TrainingArguments,\n",
        "    Trainer\n",
        ")\n",
        "from seqeval.metrics import precision_score, recall_score, f1_score, classification_report\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Get Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MacMorphoData:\n",
        "    def __init__(self, filepath, tokenizer, max_length=50):\n",
        "        self.filepath = filepath\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "        # Read lines\n",
        "        with open(self.filepath, 'r', encoding='utf-8') as f:\n",
        "            lines = f.read().split('\\n')\n",
        "\n",
        "        # Each line -> list of \"word_POS\" strings\n",
        "        items = [line.strip().split() for line in lines if line.strip()]\n",
        "\n",
        "        # Turn each sentence into a list of (word, tag) tuples\n",
        "        self.wt = [\n",
        "            [tuple(unit.split('_')) for unit in sentence]\n",
        "            for sentence in items\n",
        "        ]\n",
        "\n",
        "        # Flatten all (word, tag) pairs across the entire file\n",
        "        pairs = [(word, tag) for sentence in self.wt for (word, tag) in sentence]\n",
        "\n",
        "        # Separate words and tags\n",
        "        self.words, self.tags = zip(*pairs)  # 'words' and 'tags' are tuples now\n",
        "        self.words = list(self.words)\n",
        "        self.tags = list(self.tags)\n",
        "\n",
        "        # Build unique tags\n",
        "        self.unique_tags = sorted(list(set(self.tags)))\n",
        "\n",
        "        # Tag <-> index maps\n",
        "        self.tag2idx = {tag: idx for idx, tag in enumerate(self.unique_tags)}\n",
        "        self.idx2tag = {idx: tag for tag, idx in self.tag2idx.items()}\n",
        "\n",
        "        # Now let's preprocess (tokenize) all sentences\n",
        "        self.processed_data = self._process_all_sentences()\n",
        "        # self.processed_data is a list of dicts:\n",
        "        #   [{'input_ids':..., 'attention_mask':..., 'labels':...}, ...]\n",
        "\n",
        "    def _process_sentence(self, sentence):\n",
        "        \"\"\"\n",
        "        sentence: list of (word, tag) pairs, e.g. [(\"Jersei\",\"N\"), (\"atinge\",\"V\"), (\"média\",\"N\")]\n",
        "        returns a dict with input_ids, attention_mask, labels\n",
        "        \"\"\"\n",
        "        words = [w for (w, t) in sentence]\n",
        "        tags = [t for (w, t) in sentence]\n",
        "\n",
        "        encoding = self.tokenizer(\n",
        "            words,\n",
        "            is_split_into_words=True,\n",
        "            padding='max_length',    # or True, or 'longest'\n",
        "            truncation=True,\n",
        "            max_length=self.max_length,\n",
        "            return_tensors='pt'      # return PyTorch tensors\n",
        "        )\n",
        "\n",
        "        input_ids = encoding['input_ids'][0]        # shape (seq_len,)\n",
        "        attention_mask = encoding['attention_mask'][0]  # shape (seq_len,)\n",
        "\n",
        "        # Use word_ids() to map back to original words\n",
        "        word_ids = encoding.word_ids(batch_index=0)\n",
        "\n",
        "        # Build label_ids aligned with subwords\n",
        "        label_ids = []\n",
        "        for word_id in word_ids:\n",
        "            if word_id is None:\n",
        "                # Special tokens [CLS], [SEP], or padded subwords\n",
        "                label_ids.append(-100)\n",
        "            else:\n",
        "                # Use the word_id to find the original tag\n",
        "                original_tag = tags[word_id]\n",
        "                label_id = self.tag2idx[original_tag]\n",
        "                label_ids.append(label_id)\n",
        "\n",
        "        # Return a dictionary\n",
        "        return {\n",
        "            'input_ids': input_ids,\n",
        "            'attention_mask': attention_mask,\n",
        "            'labels': torch.tensor(label_ids)\n",
        "        }\n",
        "\n",
        "    def _process_all_sentences(self):\n",
        "        \"\"\"\n",
        "        Helper to process/tokenize all sentences in self.wt\n",
        "        \"\"\"\n",
        "        processed = []\n",
        "        for sentence in self.wt:\n",
        "            processed_example = self._process_sentence(sentence)\n",
        "            processed.append(processed_example)\n",
        "        return processed\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        Number of sentences (before tokenization).\n",
        "        \"\"\"\n",
        "        return len(self.wt)\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class POSDataset(Dataset):\n",
        "    def __init__(self, encodings):\n",
        "        self.encodings = encodings  # list of dicts\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.encodings)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {k: v for k, v in self.encodings[idx].items()}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"neuralmind/bert-base-portuguese-cased\")\n",
        "\n",
        "train_data = MacMorphoData(train_filepath, tokenizer, max_length=50)\n",
        "test_data = MacMorphoData(test_filepath, tokenizer, max_length=50)\n",
        "\n",
        "from transformers import DataCollatorForTokenClassification\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
        "\n",
        "train_dataset = POSDataset(train_data.processed_data)\n",
        "eval_dataset = POSDataset(test_data.processed_data)\n",
        "\n",
        "num_labels = len(train_data.unique_tags)\n",
        "model = AutoModelForTokenClassification.from_pretrained(\n",
        "    \"neuralmind/bert-base-portuguese-cased\",\n",
        "    num_labels=num_labels\n",
        ")\n",
        "\n",
        "# Make model aware of label mapping\n",
        "model.config.id2label = train_data.idx2tag\n",
        "model.config.label2id = train_data.tag2idx\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"results\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    logging_strategy=\"epoch\",\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    weight_decay=0.01,\n",
        "    learning_rate=5e-5\n",
        ")\n",
        "def align_predictions(predictions, label_ids):\n",
        "    preds = predictions.argmax(-1)\n",
        "    batch_size, seq_len = preds.shape\n",
        "    out_preds, out_labels = [], []\n",
        "\n",
        "    for i in range(batch_size):\n",
        "        pred_i = []\n",
        "        label_i = []\n",
        "        for j in range(seq_len):\n",
        "            if label_ids[i][j] != -100:\n",
        "                pred_i.append(model.config.id2label[preds[i][j].item()])\n",
        "                label_i.append(model.config.id2label[label_ids[i][j].item()])\n",
        "        out_preds.append(pred_i)\n",
        "        out_labels.append(label_i)\n",
        "    return out_preds, out_labels\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = torch.from_numpy(logits)\n",
        "    label_ids = torch.from_numpy(labels)\n",
        "\n",
        "    pred_tags, true_tags = align_predictions(predictions, label_ids)\n",
        "    precision = precision_score(true_tags, pred_tags)\n",
        "    recall = recall_score(true_tags, pred_tags)\n",
        "    f1 = f1_score(true_tags, pred_tags)\n",
        "    return {\"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Train and Evaluate Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "####################\n",
        "# 5) Train\n",
        "####################\n",
        "print(\"Starting training\\n\\n\")\n",
        "trainer.train()\n",
        "print(\"Training ended\\n\\n\")\n",
        "\n",
        "\n",
        "####################\n",
        "# 5) Save\n",
        "####################\n",
        "trainer.save_model(\"results\")\n",
        "\n",
        "\n",
        "####################\n",
        "# 6) Evaluate\n",
        "####################\n",
        "\n",
        "\"\"\"###Evaluate overall\"\"\"\n",
        "print(\"Starting evaluation\\n\\n\")\n",
        "metrics = trainer.evaluate()\n",
        "print(\"Overall Evaluation ended\\n\\n\")\n",
        "print(\"Overall Evaluation:\", metrics)\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "from sklearn.metrics import classification_report as sklearn_classification_report\n",
        "\n",
        "\n",
        "\"\"\"###Evaluate per class\"\"\"\n",
        "def evaluate_model_per_class(model, tokenizer, X_test, y_test, label_classes):\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "\n",
        "    y_pred = []\n",
        "    y_true = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for sentence, true_tags in tqdm(zip(X_test, y_test), total=len(X_test), desc=\"Evaluating\", unit=\"sentence\"):\n",
        "            # Tokenize input\n",
        "            inputs = tokenizer(sentence, return_tensors=\"pt\", is_split_into_words=True, truncation=True)\n",
        "            outputs = model(**inputs)\n",
        "\n",
        "            # Get predictions (logits -> argmax -> predicted labels)\n",
        "            logits = outputs.logits\n",
        "            predictions = torch.argmax(logits, dim=-1)\n",
        "\n",
        "            # Map predictions back to the original tags\n",
        "            word_ids = inputs.word_ids(batch_index=0)\n",
        "            predicted_tags = []\n",
        "            for idx, word_id in enumerate(word_ids):\n",
        "                if word_id is None:  # Skip special tokens\n",
        "                    continue\n",
        "                if word_id != word_ids[idx - 1]:  # Only consider the first subword\n",
        "                    predicted_tags.append(model.config.id2label[predictions[0, idx].item()])\n",
        "\n",
        "            # Ensure the prediction length matches the true tag length\n",
        "            if len(predicted_tags) == len(true_tags):\n",
        "                y_pred.append(predicted_tags)\n",
        "                y_true.append(true_tags)\n",
        "            else:\n",
        "                print(f\"Length mismatch: predicted {len(predicted_tags)}, true {len(true_tags)}\")\n",
        "\n",
        "    # Flatten the lists for classification report\n",
        "    y_true_flat = [tag for sentence in y_true for tag in sentence]\n",
        "    y_pred_flat = [tag for sentence in y_pred for tag in sentence]\n",
        "\n",
        "    # Generate and print the classification report\n",
        "    report = sklearn_classification_report(y_true_flat, y_pred_flat, labels=label_classes, zero_division=0)\n",
        "    print(report)\n",
        "\n",
        "    return report\n",
        "\n",
        "\n",
        "X_test = [[word for word, tag in sentence] for sentence in test_data.wt]\n",
        "y_test = [[tag for word, tag in sentence] for sentence in test_data.wt]\n",
        "\n",
        "metrics_per_class = evaluate_model_per_class(model, tokenizer, X_test, y_test, test_data.unique_tags)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save evaluation \n",
        "\n",
        "import pickle\n",
        "\n",
        "pickle_file = \"results/evaluation_metrics.pkl\"\n",
        "with open(pickle_file, \"wb\") as f:\n",
        "    pickle.dump(metrics, f)\n",
        "print(f\"\\n\\nEvaluation saved to {pickle_file}\\n\\n\")\n",
        "\n",
        "per_class_pickle_file = \"results/evaluation_metrics_per_class.pkl\"\n",
        "with open(per_class_pickle_file, \"wb\") as f:\n",
        "    pickle.dump(metrics_per_class, f)\n",
        "print(f\"\\n\\nEvaluation saved to {per_class_pickle_file}\\n\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Show results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Overall Evaluation\n",
            "{'eval_loss': 0.10592348128557205, 'eval_precision': 0.9766553452805465, 'eval_recall': 0.9778856948904728, 'eval_f1': 0.9772701328436634, 'eval_runtime': 64.4366, 'eval_samples_per_second': 154.989, 'eval_steps_per_second': 19.383, 'epoch': 3.0}\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Evaluation per class\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         ADJ       0.96      0.96      0.96      8554\n",
            "         ADV       0.93      0.93      0.93      5446\n",
            "      ADV-KS       0.84      0.86      0.85       230\n",
            "         ART       0.99      0.99      0.99     12580\n",
            "         CUR       1.00      1.00      1.00       296\n",
            "          IN       0.54      0.77      0.63        98\n",
            "          KC       0.99      0.98      0.98      4531\n",
            "          KS       0.94      0.93      0.93      2538\n",
            "           N       0.98      0.98      0.98     36542\n",
            "       NPROP       0.98      0.98      0.98     15936\n",
            "         NUM       0.97      0.96      0.97      2541\n",
            "         PCP       0.98      0.97      0.97      3640\n",
            "        PDEN       0.87      0.92      0.90      1092\n",
            "        PREP       0.98      0.99      0.98     16778\n",
            "    PREP+ADV       0.94      0.97      0.95        31\n",
            "    PREP+ART       0.99      0.99      0.99     10219\n",
            " PREP+PRO-KS       0.87      0.93      0.90        58\n",
            " PREP+PROADJ       0.99      1.00      1.00       309\n",
            "PREP+PROPESS       1.00      0.98      0.99       126\n",
            " PREP+PROSUB       0.92      0.88      0.90       156\n",
            "      PRO-KS       0.93      0.96      0.94      2195\n",
            "      PROADJ       0.97      0.98      0.97      3419\n",
            "     PROPESS       0.99      0.99      0.99      2876\n",
            "      PROSUB       0.93      0.90      0.91      1567\n",
            "          PU       1.00      1.00      1.00     26904\n",
            "           V       1.00      0.99      1.00     19711\n",
            "\n",
            "    accuracy                           0.98    178373\n",
            "   macro avg       0.94      0.95      0.95    178373\n",
            "weighted avg       0.98      0.98      0.98    178373\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import pickle\n",
        "\n",
        "pickle_file = \"results/evaluation_metrics.pkl\"\n",
        "with open(pickle_file, \"rb\") as f:\n",
        "    data_metrics_1 = pickle.load(f)\n",
        "print(f\"\\n\\nOverall Evaluation\\n{data_metrics_1}\\n\\n\")\n",
        "\n",
        "per_class_pickle_file = \"results/evaluation_metrics_per_class.pkl\"\n",
        "with open(per_class_pickle_file, \"rb\") as f:\n",
        "    data_metrics_2 = pickle.load(f)\n",
        "print(f\"\\n\\nEvaluation per class\\n{data_metrics_2}\\n\\n\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
